{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import math\n",
    "import editdistance\n",
    "import warnings\n",
    "\n",
    "# Load the MATLAB file into a Python object\n",
    "mat_data = scipy.io.loadmat('data/blogcatalog.mat')\n",
    "\n",
    "# Extract the adjacency matrix from the object\n",
    "adj_matrix = mat_data['network']\n",
    "\n",
    "group_labels = mat_data['group']\n",
    "\n",
    "# Get the index of the maximum value for each row\n",
    "labels = group_labels.argmax(axis=1).A1\n",
    "\n",
    "# Convert the adjacency matrix to a NetworkX graph\n",
    "G = nx.from_numpy_array(adj_matrix)\n",
    "\n",
    "nx.set_edge_attributes(G, nx.get_edge_attributes(G, \"weight\"), \"capacity\")\n",
    "\n",
    "# Print some basic information about the graph\n",
    "print(nx.info(G))  \n",
    "\n",
    "\n",
    "def max_flow(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        ss_weight, _ = nx.maximum_flow(G, current_node, destination)\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight\n",
    "\n",
    "def min_cost_max_flow(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        mincostFlow = nx.max_flow_min_cost(G, current_node, destination)\n",
    "        ss_weight = nx.cost_of_flow(G, mincostFlow)\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def jaccard_coefficient(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        denominator = len(curr)+len(dest)\n",
    "        if denominator == 0:\n",
    "            ss_weight = 0\n",
    "        else:\n",
    "            numerator = len(set(curr+dest))\n",
    "            ss_weight = numerator/denominator\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def adamic_adar(G, current_node, destination, num_jobs=-1):\n",
    "    def calc_weight(node):\n",
    "        denominator = math.log10(len(list(G.neighbors(node))))\n",
    "        if denominator == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 / denominator\n",
    "        \n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        intersection = set(curr+dest)\n",
    "        \n",
    "        adamic_adar_indices = sum(Parallel(n_jobs=num_jobs)(\n",
    "            delayed(calc_weight)(node) for node in intersection\n",
    "        ))\n",
    "        \n",
    "        ss_weight = adamic_adar_indices\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight\n",
    "\n",
    "def common_neighbors(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        ss_weight = len(set(curr+dest))\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def lhn_index(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        denominator = len(curr)*len(dest)\n",
    "        if denominator == 0:\n",
    "            ss_weight = 0\n",
    "        else:\n",
    "            numerator = len(set(curr+dest))\n",
    "            ss_weight = numerator/denominator\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def preferential_attachment(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        ss_weight = len(curr) * len(dest)\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def hub_promoted(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        denominator = min(len(curr),len(dest))\n",
    "        if denominator == 0:\n",
    "            ss_weight = 0\n",
    "        else:\n",
    "            numerator = len(set(curr+dest))\n",
    "            ss_weight = numerator/denominator\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def hub_depressed(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        denominator = max(len(curr),len(dest))\n",
    "        if denominator == 0:\n",
    "            ss_weight = 0\n",
    "        else:\n",
    "            numerator = len(set(curr+dest))\n",
    "            ss_weight = numerator/denominator\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def salton_index(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        denominator = math.sqrt(len(curr)*len(dest))\n",
    "        if denominator == 0:\n",
    "            ss_weight = 0\n",
    "        else:\n",
    "            numerator = len(set(curr+dest))\n",
    "            ss_weight = numerator/denominator\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def sorenson_index(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        denominator = len(curr)+len(dest)\n",
    "        if denominator == 0:\n",
    "            ss_weight = 0\n",
    "        else:\n",
    "            numerator = 2*len(set(curr+dest))\n",
    "            ss_weight = numerator/denominator\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight \n",
    "\n",
    "def resource_allocation(G, current_node, destination, num_jobs=-1):\n",
    "    def calc_weight(node):\n",
    "        denominator = len(list(G.neighbors(node)))\n",
    "        if denominator == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 / denominator\n",
    "        \n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        intersection = set(curr+dest)\n",
    "        \n",
    "        resource_allocation_indices = sum(Parallel(n_jobs=num_jobs)(\n",
    "            delayed(calc_weight)(node) for node in intersection\n",
    "        ))\n",
    "        \n",
    "        ss_weight = resource_allocation_indices\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight\n",
    "\n",
    "def levenshtein(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        ss_weight = editdistance.eval(curr, dest)\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight\n",
    "\n",
    "def tversky(G, current_node, destination):\n",
    "    if current_node != destination:\n",
    "        curr = list(G.neighbors(current_node))\n",
    "        dest = list(G.neighbors(destination))\n",
    "        a = set(curr)\n",
    "        b = set(dest)\n",
    "        union = a.union(b)\n",
    "        a_compl = a.difference(b)\n",
    "        b_compl = b.difference(a)\n",
    "        denominator = len(union) + len(curr)*len(a_compl)+len(dest)*len(b_compl)\n",
    "        if denominator == 0:\n",
    "            ss_weight = 0\n",
    "        else:\n",
    "            numerator = len(a.intersection(b))\n",
    "            ss_weight = numerator/denominator\n",
    "    else:\n",
    "        ss_weight = 0\n",
    "    return ss_weight\n",
    "\n",
    "def speed_up(G, num_workers, transition_matrix_function):\n",
    "    nodes = G.nodes\n",
    "    # Split the nodes into chunks for each worker\n",
    "    node_chunks = np.array_split(nodes, num_workers)\n",
    "\n",
    "    # Use joblib to parallelize the calculation of ss_weight\n",
    "    ss_weights = Parallel(n_jobs=num_workers)(\n",
    "        delayed(transition_matrix_function)(G, current_node, destination)\n",
    "        for chunk in node_chunks\n",
    "        for current_node in chunk\n",
    "        for destination in nodes\n",
    "    )\n",
    "\n",
    "    # Reshape the ss_weights list into a matrix\n",
    "    ss_weights_matrix1 = np.reshape(ss_weights, (len(nodes), len(nodes)))\n",
    "    # ss_weights_matrix1 = ss_weights_matrix1/ss_weights_matrix1.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    # check if row sums are zero\n",
    "    row_sums = ss_weights_matrix1.sum(axis=1)\n",
    "    zero_rows = np.where(row_sums == 0)[0]\n",
    "\n",
    "    # set all elements in zero rows to zero, except for diagonal element\n",
    "    ss_weights_matrix1[zero_rows, :] = 0\n",
    "    for i in zero_rows:\n",
    "        ss_weights_matrix1[i, i] = 1\n",
    "\n",
    "    row_sums = ss_weights_matrix1.sum(axis=1)\n",
    "    # normalize matrix by row sums\n",
    "    ss_weights_matrix1 = np.divide(ss_weights_matrix1, row_sums[:, np.newaxis])\n",
    "    return ss_weights_matrix1\n",
    "\n",
    "def sparse_speed_up(G, k, num_workers, transition_matrix_function):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "        A = nx.adjacency_matrix(G)\n",
    "    A.data = np.ones_like(A.data)\n",
    "    S = A.copy()\n",
    "    for i in range(k):\n",
    "        S = A + np.multiply(S,A)\n",
    "\n",
    "#     print(\"Sparse Matrix Shape:\")\n",
    "#     print(A.shape, S.shape, A.nnz, S.nnz)\n",
    "\n",
    "    ss_weights_matrix1 = np.zeros(S.shape)\n",
    "    for src_idx in range(S.shape[0]):\n",
    "        for dest_idx in range(S.shape[0]):\n",
    "            if S[src_idx, dest_idx] > 0:\n",
    "                ss_weights_matrix1[src_idx, dest_idx] = transition_matrix_function(G, src_idx, dest_idx)\n",
    "\n",
    "    # check if row sums are zero\n",
    "    row_sums = ss_weights_matrix1.sum(axis=1)\n",
    "    zero_rows = np.where(row_sums == 0)[0]\n",
    "\n",
    "    # set all elements in zero rows to zero, except for diagonal element\n",
    "    ss_weights_matrix1[zero_rows, :] = 0\n",
    "    for i in zero_rows:\n",
    "        ss_weights_matrix1[i, i] = 1\n",
    "\n",
    "    # normalize matrix by row sums\n",
    "    row_sums = ss_weights_matrix1.sum(axis=1)\n",
    "    ss_weights_matrix1 = np.divide(ss_weights_matrix1, row_sums[:, np.newaxis])\n",
    "\n",
    "    return ss_weights_matrix1\n",
    "\n",
    "def generate_walks(graph, num_walks, walk_length, transition_probs, num_jobs=-1):\n",
    "    \"\"\"\n",
    "    Generate random walks on the graph using the specified transition probabilities in parallel.\n",
    "\n",
    "    Parameters:\n",
    "    graph (networkx.Graph): The input graph.\n",
    "    num_walks (int): The number of random walks to generate for each node in the graph.\n",
    "    walk_length (int): The length of each random walk.\n",
    "    transition_probs (np.ndarray): A 2D numpy array of shape (num_nodes, num_nodes) containing the transition\n",
    "        probabilities between each pair of nodes in the graph.\n",
    "    num_jobs (int): Number of jobs to run in parallel. If set to -1, it will use all available cores.\n",
    "\n",
    "    Returns:\n",
    "    List of walks. Each walk is a list of nodes.\n",
    "    \"\"\"\n",
    "    walks = []\n",
    "    nodes = list(graph.nodes())\n",
    "\n",
    "    # Convert the transition probabilities to a dictionary of dictionaries for faster access\n",
    "    probs = {}\n",
    "    for i, node_i in enumerate(nodes):\n",
    "        probs[node_i] = {}\n",
    "        for j, node_j in enumerate(nodes):\n",
    "            probs[node_i][node_j] = transition_probs[i][j]\n",
    "\n",
    "    def generate_walks_for_node(node):\n",
    "        node_walks = []\n",
    "        for walk in range(num_walks):\n",
    "            walk_list = [node]\n",
    "            for step in range(walk_length - 1):\n",
    "                neighbors = list(probs[walk_list[-1]].keys())\n",
    "                probabilities = list(probs[walk_list[-1]].values())\n",
    "                next_node = np.random.choice(neighbors, p=probabilities)\n",
    "                walk_list.append(next_node)\n",
    "            node_walks.append(walk_list)\n",
    "        return node_walks\n",
    "\n",
    "    node_walks_list = Parallel(n_jobs=num_jobs)(\n",
    "        delayed(generate_walks_for_node)(node) for node in nodes)\n",
    "\n",
    "    for node_walks in node_walks_list:\n",
    "        walks += node_walks\n",
    "\n",
    "    return walks\n",
    "\n",
    "def cluster_scoring(emb, y_true, num_folds=2):\n",
    "    #Implementing cross validation\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=None)\n",
    "\n",
    "    f1_macro_score = []\n",
    "    f1_micro_score =[]\n",
    "    fm_score = []\n",
    "    acc_score = []\n",
    "    \n",
    "    for train_index , test_index in kf.split(emb, y_true):\n",
    "        X_train , X_test = emb[train_index,:], emb[test_index,:]\n",
    "        y_train , y_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "        #model = KMedoids(n_clusters=len(np.unique(y_train)), metric='euclidean').fit(X_train)\n",
    "        # Create an instance of the logistic regression classifier with L2 regularization\n",
    "        model = LogisticRegression(penalty='l2', multi_class='ovr', solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "        # Compute clustering metrics on test set\n",
    "        test_labels = model.predict(X_test)\n",
    "        f1macro_score_test = f1_score(y_test, test_labels, average='macro')\n",
    "        f1micro_score_test = f1_score(y_test, test_labels, average=\"micro\")\n",
    "        fm_score_test = fowlkes_mallows_score(y_test, test_labels)\n",
    "        acc_score_test = accuracy_score(y_test, test_labels, normalize=True)\n",
    "        \n",
    "        \n",
    "        f1_macro_score.append(f1macro_score_test)\n",
    "        f1_micro_score.append(f1micro_score_test)\n",
    "        fm_score.append(fm_score_test)\n",
    "        acc_score.append(acc_score_test)\n",
    "\n",
    "        \n",
    "    \n",
    "    avg_f1_macro_score = sum(f1_macro_score)/num_folds\n",
    "    avg_f1_micro_score = sum(f1_micro_score)/num_folds\n",
    "    avg_fm_score = sum(fm_score)/num_folds\n",
    "    avg_acc_score = sum(acc_score)/num_folds\n",
    "\n",
    "    \n",
    "    return 'Avg avg_f1_macro_score : {}'.format(avg_f1_macro_score), \\\n",
    "            'Avg avg_f1_micro_score : {}'.format(avg_f1_micro_score), \\\n",
    "            'Avg fowlkes_mallows_score : {}'.format(avg_fm_score), \\\n",
    "            'Avg avg_acc_score : {}'.format(avg_acc_score)\n",
    "\n",
    "def getnet(G,func,num_walks=10, walk_length=80, num_workers=4, window=10,dimension=128, num_folds=2):\n",
    "    start_time = time.time()\n",
    "    entry = {}\n",
    "    func_name = str(func).split()[1]\n",
    "    print(f\"Computing {func_name} transition matrix\")\n",
    "    ss_weights_matrix = speed_up(G,num_workers,func)\n",
    "    entry[\"transition_matrix\"] = ss_weights_matrix.tolist()\n",
    "    print(f\"Computing {func_name} walks\")\n",
    "    walks = generate_walks(G, num_walks=num_walks, walk_length=walk_length, transition_probs = ss_weights_matrix)\n",
    "    print(f\"Computing {func_name} model\")\n",
    "    model = Word2Vec(walks, window=window, workers=num_workers, vector_size=dimension)\n",
    "    emb=model.wv[[i for i in model.wv.key_to_index]]\n",
    "    model.save(f\"blogcatalog/my_{func_name}_model\")\n",
    "    print(f\"Computing {func_name} results\")\n",
    "    results = cluster_scoring(emb,labels,num_folds=num_folds)\n",
    "    entry[\"results\"] = results\n",
    "    end_time = time.time()\n",
    "    time_elapsed = end_time - start_time\n",
    "    entry[\"time_elapsed\"] = time_elapsed\n",
    "    with open(f\"blogcatalog/{func_name}_results.txt\", \"w\") as f:\n",
    "        json.dump(entry, f)\n",
    "    return entry\n",
    "\n",
    "def sparse_getnet(G,k,func,num_walks=10, walk_length=80, num_workers=4, window=10,dimension=128, num_folds=2):\n",
    "    start_time = time.time()\n",
    "    entry = {}\n",
    "    func_name = str(func).split()[1]\n",
    "    print(f\"Computing {func_name}{k} transition matrix\")\n",
    "    ss_weights_matrix = sparse_speed_up(G,k,num_workers,func)\n",
    "    entry[\"transition_matrix\"] = ss_weights_matrix.tolist()\n",
    "    print(f\"Computing {func_name}{k} walks\")\n",
    "    walks = generate_walks(G, num_walks=num_walks, walk_length=walk_length, transition_probs = ss_weights_matrix)\n",
    "    print(f\"Computing {func_name}{k} model\")\n",
    "    model = Word2Vec(walks, window=window, workers=num_workers, vector_size=dimension)\n",
    "    emb=model.wv[[i for i in model.wv.key_to_index]]\n",
    "    model.save(f\"Homo_sapiens/my_{func_name}{k}_model\")\n",
    "    print(f\"Computing {func_name}{k} results\")\n",
    "    results = cluster_scoring(emb,labels,num_folds=num_folds)\n",
    "    entry[\"results\"] = results\n",
    "    end_time = time.time()\n",
    "    time_elapsed = end_time - start_time\n",
    "    entry[\"time_elapsed\"] = time_elapsed\n",
    "    with open(f\"Homo_sapiens/{func_name}{k}_results.txt\", \"w\") as f:\n",
    "        json.dump(entry, f)\n",
    "    return entry\n",
    "\n",
    "def n2vec(G, dimensions=128, p=1, q=1, walk_length=80, num_walks=10, window=10, title=''):\n",
    "    from node2vec import Node2Vec\n",
    "    entry = {}\n",
    "    node2vec_model = Node2Vec(G, dimensions=dimensions, p=p, q=q, walk_length=walk_length, num_walks=num_walks, workers=4)\n",
    "    model = node2vec_model.fit(window=window)\n",
    "    emb=model.wv[[i for i in model.wv.key_to_index]]\n",
    "    results = cluster_scoring(emb,labels,num_folds=10)\n",
    "    entry[\"results\"] = results\n",
    "    return entry\n",
    "\n",
    "# print(f\"Running node2vec\")\n",
    "# result = n2vec(G)\n",
    "# with open(f\"node2vec_experiment.txt\", \"w\") as f:\n",
    "#     json.dump(result, f)\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "funcs = [adamic_adar, resource_allocation, levenshtein, tversky]\n",
    "params = []\n",
    "for func in funcs:\n",
    "    param = (G,func, 10, 80, num_workers, 10, 128, 10)\n",
    "    params.append(param)\n",
    "\n",
    "results = Parallel(n_jobs=num_workers)(delayed(getnet)(*param) for param in params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
